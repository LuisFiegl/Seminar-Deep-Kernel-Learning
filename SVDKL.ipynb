{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "from math import floor\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# fetch dataset \n",
    "real_estate_valuation = fetch_ucirepo(id=477) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_data = real_estate_valuation.data.features \n",
    "y_data = real_estate_valuation.data.targets\n",
    "y = y_data.squeeze()\n",
    "\n",
    "X = torch.tensor(X_data.values, dtype=torch.float32)\n",
    "y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "train_n = int(floor(0.8 * len(X)))\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()\n",
    "\n",
    "# Create TensorDataset and DataLoader for training and test sets\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 2))\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcessLayer(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, num_dim, grid_bounds=(-10., 10.), grid_size=64):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            num_inducing_points=grid_size, batch_shape=torch.Size([num_dim])\n",
    "        )\n",
    "\n",
    "        variational_strategy = gpytorch.variational.GridInterpolationVariationalStrategy(\n",
    "                self, grid_size=grid_size, grid_bounds=[grid_bounds],\n",
    "                variational_distribution=variational_distribution,\n",
    "            )\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(\n",
    "                    math.exp(-1), math.exp(1), sigma=0.1, transform=torch.exp\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.grid_bounds = grid_bounds\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)\n",
    "    \n",
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, feature_extractor, num_dim, grid_bounds=(-10., 10.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.gp_layer = GaussianProcessLayer(num_dim=num_dim, grid_bounds=grid_bounds)\n",
    "        self.grid_bounds = grid_bounds\n",
    "        self.num_dim = num_dim\n",
    "\n",
    "        self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(self.grid_bounds[0], self.grid_bounds[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        #print(features.shape)\n",
    "        features = self.scale_to_bounds(features)\n",
    "        features = features.transpose(-1, -2).unsqueeze(-1)\n",
    "        #print(features.shape)\n",
    "        res = self.gp_layer(features)\n",
    "        #print(res)\n",
    "        return res\n",
    "\n",
    "# Initialize model and likelihood for regression\n",
    "model = DKLModel(feature_extractor, num_dim=2)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcessLayer(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, num_dim, grid_bounds=(-10., 10.), grid_size=64):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            num_inducing_points=grid_size, batch_shape=torch.Size([num_dim])\n",
    "        )\n",
    "\n",
    "        # Our base variational strategy is a GridInterpolationVariationalStrategy,\n",
    "        # which places variational inducing points on a Grid\n",
    "        # We wrap it with a IndependentMultitaskVariationalStrategy so that our output is a vector-valued GP\n",
    "        variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
    "            gpytorch.variational.GridInterpolationVariationalStrategy(\n",
    "                self, grid_size=grid_size, grid_bounds=[grid_bounds],\n",
    "                variational_distribution=variational_distribution,\n",
    "            ), num_tasks=num_dim,\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(\n",
    "                    math.exp(-1), math.exp(1), sigma=0.1, transform=torch.exp\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.grid_bounds = grid_bounds\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)\n",
    "    \n",
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, feature_extractor, num_dim, grid_bounds=(-10., 10.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.gp_layer = GaussianProcessLayer(num_dim=num_dim, grid_bounds=grid_bounds)\n",
    "        self.grid_bounds = grid_bounds\n",
    "        self.num_dim = num_dim\n",
    "\n",
    "        # This module will scale the NN features so that they're nice values\n",
    "        self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(self.grid_bounds[0], self.grid_bounds[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = self.scale_to_bounds(features)\n",
    "        # This next line makes it so that we learn a GP for each feature\n",
    "        features = features.transpose(-1, -2).unsqueeze(-1)\n",
    "        res = self.gp_layer(features)\n",
    "        return res\n",
    "\n",
    "model = DKLModel(feature_extractor, num_dim=data_dim)\n",
    "likelihood = gpytorch.likelihoods.SoftmaxLikelihood(num_features=model.num_dim, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) Minibatch: 100%|██████████| 2/2 [00:00<00:00, 54.06it/s, loss=5.45]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2285,  1.5363,  1.5399,  1.5280,  1.5391,  1.4911,  1.5365,  1.5395,\n",
      "          1.5391,  1.5394,  1.5340,  1.5399,  1.5370,  1.3927,  1.5397,  1.4998,\n",
      "          1.2285,  1.5395,  1.5399,  1.5378,  1.4706,  1.4313,  1.4729,  1.5163,\n",
      "          1.5388,  1.5397,  1.5391,  1.5397,  1.4534,  1.5399,  1.5394,  1.5292,\n",
      "          1.5399,  1.5363,  1.4648,  1.4949,  1.4934,  1.5390,  1.4703,  1.5394,\n",
      "          1.5392,  1.5398,  1.5394,  1.5387,  1.5237,  1.4996,  1.5399,  1.5367,\n",
      "          1.5393,  1.5394,  1.5398,  1.3451,  1.5377,  1.2813,  1.5399,  1.5397,\n",
      "          1.4760,  1.4654,  1.5399,  1.5388,  1.4866,  1.5383,  1.5292,  1.2727,\n",
      "          1.5376,  1.5305,  1.5234,  1.4996,  1.5398,  1.5379,  1.4710,  1.5397,\n",
      "          1.5398,  1.5394,  1.5398,  1.5389,  1.4534,  1.4710,  1.2794,  1.5399,\n",
      "          1.5387,  1.5399,  1.5399],\n",
      "        [ 0.1517,  0.0797,  0.2066,  0.0328,  0.1422,  0.0050,  0.0813,  0.1581,\n",
      "          0.1436,  0.1552,  0.0715,  0.2146,  0.0856,  0.1339,  0.1776,  0.0138,\n",
      "          0.1990,  0.1575,  0.2066,  0.0975,  0.0164, -0.0414,  0.0145,  0.0439,\n",
      "          0.1298,  0.1703,  0.1418,  0.1753, -0.0036,  0.2171,  0.1533,  0.0335,\n",
      "          0.2044,  0.0800,  0.0086,  0.0091,  0.0074,  0.1390,  0.0164,  0.1516,\n",
      "          0.1470,  0.1862,  0.1543,  0.1277,  0.0237,  0.0136,  0.2439,  0.0826,\n",
      "          0.1507,  0.1515,  0.1944, -0.2298,  0.0948,  0.8155,  0.2193,  0.1775,\n",
      "          0.0099,  0.0099,  0.1990,  0.1320,  0.0027,  0.1110,  0.0335, -0.2372,\n",
      "          0.0934,  0.0384,  0.0235,  0.0136,  0.1820,  0.0998,  0.0163,  0.1771,\n",
      "          0.1850,  0.1515,  0.1954,  0.1333, -0.0037,  0.0163,  0.3563,  0.2146,\n",
      "          0.1278,  0.2065,  0.2142]])\n",
      "Test set: Mean Absolute Error: 75.0145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\12lui\\AppData\\Local\\Temp\\ipykernel_12584\\190988355.py:45: UserWarning: Using a target size (torch.Size([83])) that is different to the input size (torch.Size([2, 83])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  total_loss += F.l1_loss(mean, target, reduction='sum').item()\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torch.nn.functional as F\n",
    "n_epochs = 1\n",
    "lr = 0.1\n",
    "optimizer = SGD([\n",
    "    {'params': model.feature_extractor.parameters(), 'weight_decay': 1e-4},\n",
    "    {'params': model.gp_layer.hyperparameters(), 'lr': lr * 0.01},\n",
    "    {'params': model.gp_layer.variational_parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=lr, momentum=0.9, nesterov=True, weight_decay=0)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[0.5 * n_epochs, 0.75 * n_epochs], gamma=0.1)\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=len(train_loader.dataset))\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    minibatch_iter = tqdm(train_loader, desc=f\"(Epoch {epoch}) Minibatch\")\n",
    "    with gpytorch.settings.num_likelihood_samples(8):\n",
    "        for data, target in minibatch_iter:\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = -mll(output, target)\n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad(), gpytorch.settings.num_likelihood_samples(16):\n",
    "        for data, target in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = likelihood(model(data))  # This gives us 16 samples from the predictive distribution\n",
    "            mean = output.mean\n",
    "            print(mean)\n",
    "            total_loss += F.l1_loss(mean, target, reduction='sum').item()\n",
    "            num_samples += target.size(0)\n",
    "    mae = total_loss / num_samples\n",
    "    print('Test set: Mean Absolute Error: {:.4f}'.format(mae))\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    with gpytorch.settings.use_toeplitz(False):\n",
    "        train(epoch)\n",
    "        test()\n",
    "    scheduler.step()\n",
    "    state_dict = model.state_dict()\n",
    "    likelihood_state_dict = likelihood.state_dict()\n",
    "    torch.save({'model': state_dict, 'likelihood': likelihood_state_dict}, 'dkl_checkpoint.dat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewTrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
